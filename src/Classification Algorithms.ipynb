{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Algorithms\n",
    "#### By Zachary Austin Ellis & Jose Carlos Gomez-Vazquez\n",
    "\n",
    "In this notebook we will implement our own version of the Decision Tree Classifier, Random Forest Classifer, and Naive Bayes Classifier then compare their performance against SciKit-Learn's implementations.\n",
    "For these algorithms, the Red Wine Quality dataset provided by Kaggle will be used.\n",
    "\n",
    "Note-1: The algorithms are not well optimized due to the fact that Jupyter notebooks do not support the multiprocessing module. While there is a workaround, it was mentioned that having multiple python modules is not recommended for this project. Cython would also help with performance, but requires an additional dependancy, again not recommended for this project.\n",
    "\n",
    "Note-2: Decision Tree training time is ~10 minutes. Random Forest training time is ~N_trees * 10 minutes\n",
    "\n",
    "\n",
    "Data source: https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from random import randrange\n",
    "from math import sqrt, exp, pi, floor\n",
    "\n",
    "# Used to compare our implementation's performance\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\t\t\t\tRed Wine Data\n",
      "       fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0               7.4             0.700         0.00             1.9      0.076   \n",
      "1               7.8             0.880         0.00             2.6      0.098   \n",
      "2               7.8             0.760         0.04             2.3      0.092   \n",
      "3              11.2             0.280         0.56             1.9      0.075   \n",
      "4               7.4             0.700         0.00             1.9      0.076   \n",
      "...             ...               ...          ...             ...        ...   \n",
      "1594            6.2             0.600         0.08             2.0      0.090   \n",
      "1595            5.9             0.550         0.10             2.2      0.062   \n",
      "1596            6.3             0.510         0.13             2.3      0.076   \n",
      "1597            5.9             0.645         0.12             2.0      0.075   \n",
      "1598            6.0             0.310         0.47             3.6      0.067   \n",
      "\n",
      "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "1                    25.0                  67.0  0.99680  3.20       0.68   \n",
      "2                    15.0                  54.0  0.99700  3.26       0.65   \n",
      "3                    17.0                  60.0  0.99800  3.16       0.58   \n",
      "4                    11.0                  34.0  0.99780  3.51       0.56   \n",
      "...                   ...                   ...      ...   ...        ...   \n",
      "1594                 32.0                  44.0  0.99490  3.45       0.58   \n",
      "1595                 39.0                  51.0  0.99512  3.52       0.76   \n",
      "1596                 29.0                  40.0  0.99574  3.42       0.75   \n",
      "1597                 32.0                  44.0  0.99547  3.57       0.71   \n",
      "1598                 18.0                  42.0  0.99549  3.39       0.66   \n",
      "\n",
      "      alcohol  quality  \n",
      "0         9.4        5  \n",
      "1         9.8        5  \n",
      "2         9.8        5  \n",
      "3         9.8        6  \n",
      "4         9.4        5  \n",
      "...       ...      ...  \n",
      "1594     10.5        5  \n",
      "1595     11.2        6  \n",
      "1596     11.0        6  \n",
      "1597     10.2        5  \n",
      "1598     11.0        6  \n",
      "\n",
      "[1599 rows x 12 columns]\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(df, test_size=0):\n",
    "    if test_size == 0:\n",
    "        test_size = floor( len(df.index) * 0.3 )\n",
    "\n",
    "    testing_set = df.sample(test_size)\n",
    "    df.drop(index=testing_set.index, inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    X_train = df.drop(columns=\"quality\")\n",
    "    X_test = testing_set.drop(columns=\"quality\").reset_index(drop=True)\n",
    "    \n",
    "    Y_train = df[\"quality\"]\n",
    "    Y_test = testing_set[\"quality\"].reset_index(drop=True)\n",
    "    \n",
    "    return X_train, X_test, Y_train, Y_test\n",
    "\n",
    "\n",
    "wine_data = pd.read_csv(\"../data/winequality-red.csv\")\n",
    "print(\"\\t\\t\\t\\t\\tRed Wine Data\\n\", wine_data)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(wine_data.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_counts(data):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"parameter `data` must be a Pandas DataFrame\")\n",
    "\n",
    "    labels = data.quality.unique()\n",
    "    counts = {}\n",
    "    for label in labels:\n",
    "        counts[str(label)] = len(data[data['quality'] == label].index)\n",
    "\n",
    "    return counts\n",
    "\n",
    "def gini(data):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"parameter `data` must be a Pandas DataFrame\")\n",
    "\n",
    "    counts = label_counts(data)\n",
    "    impurity = 1.0\n",
    "    for label in counts:\n",
    "        probability = counts[label] / len(data.index)\n",
    "        impurity -= probability**2\n",
    "\n",
    "    return impurity\n",
    "\n",
    "def info_gain(left_branch, right_branch, current_uncertainty):\n",
    "    if not isinstance(left_branch, pd.DataFrame):\n",
    "        raise TypeError(\"parameter `left_branch` must be a Pandas DataFrame\")\n",
    "    if not isinstance(right_branch, pd.DataFrame):\n",
    "        raise TypeError(\"parameter `right_branch` must be a Pandas DataFrame\")\n",
    "    if not isinstance(current_uncertainty, float):\n",
    "        raise TypeError(\"parameter `current_uncertainty` must be a floating point number\")\n",
    "\n",
    "    probability = len(left_branch.index) / (len(left_branch.index) + len(right_branch.index))\n",
    "    return current_uncertainty - probability * gini(left_branch) - (1 - probability) * gini(right_branch)\n",
    "\n",
    "def split(data, feature, split_point):\n",
    "    if not isinstance(data, pd.DataFrame):\n",
    "        raise TypeError(\"parameter `data` must be a Pandas DataFrame\")\n",
    "    if not isinstance(feature, str):\n",
    "        raise TypeError(\"parameter `feature` must be a String\")\n",
    "    if not isinstance(split_point, float):\n",
    "        raise TypeError(\"parameter `split_point` must be a floating point number\")\n",
    "\n",
    "    true_branch = {}\n",
    "    false_branch = {}\n",
    "    for value in data.iterrows():\n",
    "        if value[1][feature] >= split_point:\n",
    "            true_branch[value[0]] = value[1]\n",
    "        else:\n",
    "            false_branch[value[0]] = value[1]\n",
    "\n",
    "    return pd.DataFrame.from_dict(true_branch, orient='index').reset_index(drop=True), pd.DataFrame.from_dict(false_branch, orient='index').reset_index(drop=True)\n",
    "\n",
    "\n",
    "class Node:\n",
    "    \"\"\"\n",
    "    Represents a node on a decision tree\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, is_leaf, **kwargs):\n",
    "        if not isinstance(is_leaf, bool):\n",
    "            raise TypeError(\"parameter `is_leaf` must be a Boolean\")\n",
    "\n",
    "        # Leaf Node requirements\n",
    "        if is_leaf:\n",
    "            if 'predictions' not in kwargs:\n",
    "                raise ValueError(\"parameter `predictions` is required for leaf nodes\")\n",
    "            if not isinstance(kwargs['predictions'], pd.DataFrame):\n",
    "                raise TypeError(\"parameter `predictions` must be a Pandas DataFrame\")\n",
    "\n",
    "        # Decision Node requirements\n",
    "        else:\n",
    "            if 'true_branch' not in kwargs:\n",
    "                raise ValueError(\"parameter `true_branch` is required for non-leaf nodes\")\n",
    "            if 'false_branch' not in kwargs:\n",
    "                raise ValueError(\"parameter `false_branch` is required for non-leaf nodes\")\n",
    "            if 'split_point' not in kwargs:\n",
    "                raise ValueError(\"parameter `split_point` is required for non-leaf nodes\")\n",
    "\n",
    "            if not isinstance(kwargs['true_branch'], Node):\n",
    "                raise TypeError(\"parameter `true_branch` must be a Node\")\n",
    "            if not isinstance(kwargs['false_branch'], Node):\n",
    "                raise TypeError(\"parameter `false_branch` must be a Node\")\n",
    "            if not isinstance(kwargs['split_point'], tuple):\n",
    "                raise TypeError(\"parameter `split_point` must be a tuple (feature, value)\")\n",
    "            if not isinstance(kwargs['split_point'][0], str):\n",
    "                raise TypeError(\"parameter `split_point[0]` must be a String ([feature], value)\")\n",
    "            if not isinstance(kwargs['split_point'][1], float):\n",
    "                raise TypeError(\"parameter `split_point[1]` must be a float (feature, [value])\")\n",
    "\n",
    "        self.is_leaf = is_leaf\n",
    "        if is_leaf:\n",
    "            self.predictions = kwargs['predictions']\n",
    "            \n",
    "            counts = label_counts(self.predictions)\n",
    "            if len(counts) == 1:\n",
    "                self.label = int(self.predictions['quality'][0])\n",
    "            else:\n",
    "                most = 0\n",
    "                self.label = None\n",
    "                for label in counts:\n",
    "                    if counts[label] > most:\n",
    "                        most = counts[label]\n",
    "                        self.label = int(label)\n",
    "\n",
    "        else:\n",
    "            self.true_branch = kwargs['true_branch']\n",
    "            self.false_branch = kwargs['false_branch']\n",
    "            self.split_point = kwargs['split_point']\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.is_leaf:\n",
    "            return f\"Leaf Node\\n\\n{self.predictions}\\n\"\n",
    "        else:\n",
    "            return f\"Decision Node: Split at feature `{self.split_point[0]}` with value {self.split_point[1]}\\n{self.true_branch}{self.false_branch}\\n\"\n",
    "\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self):\n",
    "        self.tree = None\n",
    "        self.original_data = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"parameter `X` must be a Pandas DataFrame\")\n",
    "        if not isinstance(Y, pd.DataFrame) and not isinstance(Y, pd.Series):\n",
    "            raise TypeError(\"parameter `Y` must be a Pandas DataFrame or Pandas Series\")\n",
    "\n",
    "        self.original_data = (X, Y)\n",
    "\n",
    "        # Re-merge quality row into dataset (not-ideal, but necessary for this implementaion)\n",
    "        data = X.copy()\n",
    "        data.insert(len(data.columns), \"quality\", Y)\n",
    "        self.tree = self.__build_tree(data)\n",
    "\n",
    "        return\n",
    "\n",
    "    def predict(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"parameter `X` must be a Pandas DataFrame\")\n",
    "\n",
    "        return [self.__classify(data[1], self.tree) for data in X.iterrows()]\n",
    "\n",
    "    def score(self, Y, **kwargs):\n",
    "        if not isinstance(Y, list) and not isinstance(Y, pd.Series):\n",
    "            raise TypeError(\"parameter `Y` must be a Pandas Series or a list\")\n",
    "\n",
    "        # Get predictions, either by calling predict or if passed as argument\n",
    "        predictions = None\n",
    "        if 'X' in kwargs:\n",
    "            if not isinstance(kwargs['X'], pd.DataFrame):\n",
    "                raise TypeError(\"parameter `X` must be a Pandas DataFrame\")\n",
    "            predictions = self.predict(kwargs['X'])\n",
    "\n",
    "        elif 'predictions' in kwargs:\n",
    "            if not isinstance(kwargs['predictions'], list) and not isinstance(kwargs['predictions'], pd.Series):\n",
    "                raise TypeError(\"parameter `predictions` must be a Pandas Series or a list\")\n",
    "            predictions = kwargs['predictions']\n",
    "\n",
    "        if isinstance(Y, pd.Series):\n",
    "            Y = Y.to_list()\n",
    "        if isinstance(predictions, pd.Series):\n",
    "            predictions = predictions.tolist()\n",
    "        if len(Y) != len(predictions):\n",
    "            raise IndexError(\"parameter `Y` and predictions must have the same shape\");\n",
    "\n",
    "        correct = 0.0\n",
    "        total = len(Y)\n",
    "        for i in range(total):\n",
    "            if Y[i] == predictions[i]:\n",
    "                correct += 1\n",
    "        return correct / total\n",
    "\n",
    "    def __build_tree(self, data):\n",
    "        split_point, gain = self.__best_split(data)\n",
    "\n",
    "        if gain == 0:\n",
    "            return Node(True, predictions=data)\n",
    "\n",
    "        true_data, false_data = split(data, split_point[0], split_point[1])\n",
    "        true_branch = self.__build_tree(true_data)\n",
    "        false_branch = self.__build_tree(false_data)\n",
    "\n",
    "        return Node(False, true_branch=true_branch, false_branch=false_branch, split_point=split_point)\n",
    "\n",
    "    def __best_split(self, data):\n",
    "        if not isinstance(data, pd.DataFrame):\n",
    "            raise TypeError(\"parameter `data` must be a Pandas DataFrame\")\n",
    "\n",
    "        best_info_gain = 0.0\n",
    "        best_split_point = None\n",
    "        current_uncertainty = gini(data)\n",
    "\n",
    "        for feature in data:\n",
    "            # Dont split on quality (again, not-ideal)\n",
    "            if feature == 'quality':\n",
    "                continue\n",
    "\n",
    "            values = data[feature].unique()\n",
    "            for value in values:\n",
    "                true_branch, false_branch = split(data, feature, value)\n",
    "\n",
    "                # Skip split if no split occurred\n",
    "                if len(true_branch.index) == 0 or len(false_branch.index) == 0:\n",
    "                    continue\n",
    "\n",
    "                gain = info_gain(true_branch, false_branch, current_uncertainty)\n",
    "                if gain >= best_info_gain:\n",
    "                    best_info_gain = gain\n",
    "                    best_split_point = (feature, value)\n",
    "\n",
    "        return best_split_point, best_info_gain\n",
    "\n",
    "    def __classify(self, data, node):\n",
    "        if not isinstance(data, pd.Series):\n",
    "            raise TypeError(\"parameter `data` must be a Pandas Series\")\n",
    "        if not isinstance(node, Node):\n",
    "            raise TypeError(\"parameter `node` must be a Node\")\n",
    "\n",
    "        if node.is_leaf:\n",
    "            return node.label\n",
    "\n",
    "        feature, value = node.split_point\n",
    "        if data[feature] >= value:\n",
    "            return self.__classify(data, node.true_branch)\n",
    "        else:\n",
    "            return self.__classify(data, node.false_branch)\n",
    "\n",
    "    def __repr__(self):\n",
    "        if self.tree != None:\n",
    "            return f\"{self.tree}\"\n",
    "        else:\n",
    "            return \"Decision Tree has not been trained\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions A\n",
      " [5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 5, 6, 5, 5, 5, 6, 7, 4, 5, 6, 7, 6, 6, 5, 5, 5, 5, 5, 5, 6, 4, 5, 5, 6, 6, 6, 6, 5, 5, 7, 6, 6, 5, 6, 6, 5, 5, 6, 5, 4, 5, 5, 5, 5, 5, 6, 6, 6, 7, 6, 6, 6, 5, 6, 6, 5, 5, 5, 4, 5, 5, 6, 6, 5, 6, 5, 5, 5, 5, 6, 7, 5, 6, 5, 4, 5, 6, 6, 5, 5, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 7, 5, 6, 6, 7, 6, 6, 5, 6, 6, 6, 6, 5, 5, 5, 6, 6, 7, 5, 6, 5, 5, 5, 7, 5, 6, 6, 5, 5, 6, 6, 6, 6, 6, 5, 7, 6, 5, 6, 5, 5, 5, 7, 6, 7, 6, 7, 5, 7, 7, 7, 5, 6, 6, 5, 5, 5, 6, 6, 5, 5, 6, 6, 7, 6, 6, 6, 7, 5, 5, 5, 6, 6, 5, 6, 5, 5, 6, 5, 5, 7, 7, 5, 5, 6, 5, 6, 6, 5, 7, 5, 7, 5, 5, 6, 6, 6, 5, 7, 5, 5, 5, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 6, 6, 5, 6, 5, 5, 6, 6, 5, 6, 5, 6, 5, 5, 5, 5, 6, 5, 5, 6, 7, 5, 5, 5, 6, 7, 5, 6, 6, 6, 4, 6, 4, 6, 7, 5, 6, 5, 5, 6, 6, 7, 6, 6, 5, 7, 6, 6, 5, 5, 5, 7, 6, 5, 7, 6, 5, 5, 7, 5, 5, 6, 6, 5, 7, 6, 5, 7, 6, 7, 5, 5, 6, 5, 5, 6, 6, 5, 5, 5, 7, 5, 7, 7, 7, 5, 5, 5, 6, 4, 5, 6, 6, 7, 6, 5, 5, 5, 6, 5, 5, 6, 5, 7, 5, 6, 7, 5, 5, 5, 7, 5, 4, 5, 6, 6, 4, 6, 4, 6, 6, 6, 5, 5, 5, 5, 7, 6, 6, 5, 6, 5, 7, 5, 6, 5, 7, 7, 5, 5, 5, 6, 7, 6, 6, 5, 6, 7, 7, 5, 6, 5, 6, 5, 5, 6, 7, 6, 5, 6, 6, 7, 6, 6, 6, 6, 5, 5, 4, 6, 6, 7, 5, 6, 6, 6, 6, 7, 7, 4, 6, 6, 6, 5, 6, 7, 5, 5, 5, 5, 7, 5, 6, 7, 6, 6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 7, 5, 5, 5, 5, 5, 6, 6, 5, 7, 5, 6, 7, 7, 7, 7, 6, 4, 6, 5, 6, 5, 6, 7, 6, 7, 6, 6, 6, 7, 5, 5, 6, 5, 5, 6, 5, 7, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 7, 5, 5, 6, 6]\n",
      "Predictions B\n",
      " [5 6 6 6 6 6 6 6 7 7 5 7 5 5 5 6 7 4 5 6 5 6 6 6 5 5 5 5 5 6 4 5 5 6 6 6 6\n",
      " 5 5 7 6 6 5 6 6 5 5 6 7 7 5 5 5 5 5 6 6 6 7 6 6 6 5 6 6 7 5 7 5 5 5 5 6 5\n",
      " 6 5 5 5 5 6 7 5 6 6 4 5 6 6 5 5 5 6 7 6 5 6 6 6 6 6 6 6 8 6 6 7 5 6 6 7 6\n",
      " 6 5 6 6 6 6 5 5 5 6 6 8 5 6 5 5 5 7 5 6 6 5 5 6 6 6 6 6 5 5 6 5 6 5 5 5 7\n",
      " 6 7 6 5 5 7 7 7 5 5 6 5 5 5 5 6 5 5 6 6 7 6 6 6 7 5 5 5 6 6 5 6 5 5 6 5 5\n",
      " 6 7 5 5 5 5 6 6 5 7 5 7 5 5 6 6 6 5 7 5 5 5 6 6 5 5 7 5 5 5 5 5 6 5 5 6 6\n",
      " 8 5 6 5 5 6 6 5 6 5 5 5 5 5 5 6 5 5 6 6 5 5 5 7 7 5 6 6 6 5 6 6 6 7 5 6 5\n",
      " 5 6 6 6 6 6 5 7 6 6 5 5 6 7 6 5 5 6 6 5 5 5 5 6 6 4 7 6 5 7 6 7 5 5 6 5 5\n",
      " 6 6 5 5 5 7 5 7 7 7 5 5 5 6 6 5 6 6 7 6 5 5 6 6 5 5 7 6 7 5 6 7 5 5 5 7 5\n",
      " 5 5 6 6 7 6 4 5 6 6 6 5 5 5 5 6 6 5 5 5 7 5 6 5 5 5 5 5 5 6 7 6 6 5 6 7 7\n",
      " 6 7 5 6 5 5 6 7 6 5 6 6 6 6 6 6 6 5 5 4 6 6 6 5 6 6 5 6 7 7 5 6 6 6 5 6 5\n",
      " 5 5 5 5 7 5 6 7 6 6 5 5 5 6 5 5 5 6 6 7 5 5 5 5 5 6 6 5 7 5 6 7 7 7 7 6 4\n",
      " 6 5 6 5 7 5 6 7 6 6 6 7 5 5 6 5 5 6 8 7 5 6 6 6 6 5 5 6 6 6 5 5 5 6 6]\n",
      "Accuracy (Ellis implementation): 59.92 %\n",
      "Accuracy (Skikit-Learn implementation): 60.75 %\n"
     ]
    }
   ],
   "source": [
    "# Ellis implementation\n",
    "DecisionTreeA = DecisionTree()\n",
    "DecisionTreeA.fit(X_train, Y_train)\n",
    "DT_predictionsA = DecisionTreeA.predict(X_test)\n",
    "DT_accuracyA = round(DecisionTreeA.score(Y_test, predictions=DT_predictionsA) * 100, 2)\n",
    "\n",
    "# Scikit-Learn implementation\n",
    "DecisionTreeB = DecisionTreeClassifier()\n",
    "DecisionTreeB = DecisionTreeB.fit(X_train, Y_train)\n",
    "DT_predictionsB = DecisionTreeB.predict(X_test)\n",
    "DT_accuracyB = round(DecisionTreeB.score(X_test, Y_test) * 100, 2)\n",
    "\n",
    "# The first print function will print out each node (omitted)\n",
    "# Since the tree is not pruned, this will print a long output\n",
    "\n",
    "# print(\"Decision Tree (Ellis implementation)\\n\\n\", DecisionTreeA)\n",
    "print(\"Predictions A\\n\", DT_predictionsA)\n",
    "print(\"Predictions B\\n\", DT_predictionsB)\n",
    "print(\"Accuracy (Ellis implementation):\", DT_accuracyA, \"%\")\n",
    "print(\"Accuracy (Skikit-Learn implementation):\", DT_accuracyB, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForest:\n",
    "    def __init__(self, n_trees=5):\n",
    "        if not isinstance(n_trees, int):\n",
    "            raise TypeError(\"parameter `n_trees` must be an integer\")\n",
    "\n",
    "        self.n_trees = n_trees\n",
    "        self.trees = None\n",
    "        self.original_data = None\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"parameter `X` must be a Pandas DataFrame\")\n",
    "        if not isinstance(Y, pd.DataFrame) and not isinstance(Y, pd.Series):\n",
    "            raise TypeError(\"parameter `Y` must be a Pandas DataFrame or Pandas Series\")\n",
    "\n",
    "        self.original_data = (X, Y)\n",
    "        self.trees = []\n",
    "        for i in range(self.n_trees):\n",
    "            tree = DecisionTree()\n",
    "            tree.fit(X, Y)\n",
    "            self.trees.append(tree)\n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            raise TypeError(\"parameter `X` must be a Pandas DataFrame\")\n",
    "\n",
    "        tree_results = [tree.predict(X) for tree in self.trees]\n",
    "        predictions = [None] * len(X.index)\n",
    "        for i in range(len(X.index)):\n",
    "            result_counts = {}\n",
    "\n",
    "            for result in tree_results:\n",
    "                prediction = str(result[i])\n",
    "                if prediction not in result_counts:\n",
    "                    result_counts[prediction] = 0\n",
    "                result_counts[prediction] += 1\n",
    "\n",
    "            most = 0\n",
    "            for prediction in result_counts:\n",
    "                if result_counts[prediction] > most:\n",
    "                    most = result_counts[prediction]\n",
    "                    predictions[i] = int(prediction)\n",
    "\n",
    "        return predictions\n",
    "    \n",
    "    def score(self, Y, **kwargs):\n",
    "        if not isinstance(Y, list) and not isinstance(Y, pd.Series):\n",
    "            raise TypeError(\"parameter `Y` must be a Pandas Series or a list\")\n",
    "\n",
    "        # Get predictions, either by calling predict or if passed as argument\n",
    "        predictions = None\n",
    "        if 'X' in kwargs:\n",
    "            if not isinstance(kwargs['X'], pd.DataFrame):\n",
    "                raise TypeError(\"parameter `X` must be a Pandas DataFrame\")\n",
    "            predictions = self.predict(kwargs['X'])\n",
    "\n",
    "        elif 'predictions' in kwargs:\n",
    "            if not isinstance(kwargs['predictions'], list):\n",
    "                raise TypeError(\"parameter `predictions` must be a list\")\n",
    "            predictions = kwargs['predictions']\n",
    "\n",
    "        if isinstance(Y, pd.Series):\n",
    "            Y = Y.to_list()\n",
    "        if len(Y) != len(predictions):\n",
    "            raise IndexError(\"parameter `Y` and predictions must have the same shape\");\n",
    "\n",
    "        correct = 0.0\n",
    "        total = len(Y)\n",
    "        for i in range(total):\n",
    "            if Y[i] == predictions[i]:\n",
    "                correct += 1\n",
    "        return correct / total\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"\n",
    "        DO NOT PRINT RANDOM FOREST TREE! Consider this a warning.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.trees != None:\n",
    "            ret = \"\"\n",
    "            i = 1\n",
    "            for tree in self.trees:\n",
    "                ret += f\"Decision Tree {i}\\n{tree}\"\n",
    "                i += 1\n",
    "            return ret\n",
    "        else:\n",
    "            return \"Random Forest Tree has not been trained\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions A\n",
      " [5, 6, 6, 6, 6, 6, 6, 6, 7, 7, 5, 6, 5, 5, 5, 6, 7, 4, 5, 6, 7, 6, 6, 5, 5, 5, 5, 5, 5, 6, 4, 5, 5, 6, 6, 6, 6, 5, 5, 7, 6, 6, 5, 6, 6, 5, 5, 6, 5, 4, 5, 5, 5, 5, 5, 6, 6, 6, 7, 6, 6, 6, 5, 6, 6, 5, 5, 5, 4, 5, 5, 6, 6, 5, 6, 5, 5, 5, 5, 6, 7, 5, 6, 5, 4, 5, 6, 6, 5, 5, 5, 6, 6, 6, 5, 6, 6, 6, 6, 6, 6, 6, 8, 6, 6, 7, 5, 6, 6, 7, 6, 6, 5, 6, 6, 6, 6, 5, 5, 5, 6, 6, 7, 5, 6, 5, 5, 5, 7, 5, 6, 6, 5, 5, 6, 6, 6, 6, 6, 5, 7, 6, 5, 6, 5, 5, 5, 7, 6, 7, 6, 7, 5, 7, 7, 7, 5, 6, 6, 5, 5, 5, 6, 6, 5, 5, 6, 6, 7, 6, 6, 6, 7, 5, 5, 5, 6, 6, 5, 6, 5, 5, 6, 5, 5, 7, 7, 5, 5, 6, 5, 6, 6, 5, 7, 5, 7, 5, 5, 6, 6, 6, 5, 7, 5, 5, 5, 6, 6, 6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 5, 6, 6, 6, 5, 6, 5, 5, 6, 6, 5, 6, 5, 6, 5, 5, 5, 5, 6, 5, 5, 6, 7, 5, 5, 5, 6, 7, 5, 6, 6, 6, 4, 6, 4, 6, 7, 5, 6, 5, 5, 6, 6, 7, 6, 6, 5, 7, 6, 6, 5, 5, 5, 7, 6, 5, 7, 6, 5, 5, 7, 5, 5, 6, 6, 5, 7, 6, 5, 7, 6, 7, 5, 5, 6, 5, 5, 6, 6, 5, 5, 5, 7, 5, 7, 7, 7, 5, 5, 5, 6, 4, 5, 6, 6, 7, 6, 5, 5, 5, 6, 5, 5, 6, 5, 7, 5, 6, 7, 5, 5, 5, 7, 5, 4, 5, 6, 6, 4, 6, 4, 6, 6, 6, 5, 5, 5, 5, 7, 6, 6, 5, 6, 5, 7, 5, 6, 5, 7, 7, 5, 5, 5, 6, 7, 6, 6, 5, 6, 7, 7, 5, 6, 5, 6, 5, 5, 6, 7, 6, 5, 6, 6, 7, 6, 6, 6, 6, 5, 5, 4, 6, 6, 7, 5, 6, 6, 6, 6, 7, 7, 4, 6, 6, 6, 5, 6, 7, 5, 5, 5, 5, 7, 5, 6, 7, 6, 6, 5, 5, 5, 5, 5, 5, 5, 6, 5, 7, 5, 5, 5, 5, 5, 6, 6, 5, 7, 5, 6, 7, 7, 7, 7, 6, 4, 6, 5, 6, 5, 6, 7, 6, 7, 6, 6, 6, 7, 5, 5, 6, 5, 5, 6, 5, 7, 5, 6, 6, 6, 6, 5, 5, 6, 6, 6, 7, 5, 5, 6, 6]\n",
      "Predictions B\n",
      " [5 6 6 7 6 6 6 5 6 6 5 6 5 5 5 6 6 5 5 5 5 6 6 5 6 5 5 5 5 6 5 5 5 6 6 6 5\n",
      " 5 5 7 6 6 5 6 6 5 6 6 6 6 5 5 5 5 5 5 6 5 6 6 6 4 5 6 6 6 5 6 6 5 6 6 6 5\n",
      " 6 5 5 5 5 6 7 5 6 6 4 5 5 6 5 5 5 6 6 6 5 6 5 6 5 5 6 6 7 6 6 7 5 5 6 7 6\n",
      " 6 5 6 6 6 6 5 5 5 6 6 6 5 5 5 5 5 6 5 6 6 5 5 5 5 7 6 5 5 6 5 5 5 6 5 5 7\n",
      " 6 7 6 6 5 5 7 7 5 5 5 5 6 5 5 6 5 5 6 7 7 6 6 5 6 5 5 5 5 6 6 6 6 6 6 5 6\n",
      " 6 7 5 5 5 5 6 6 5 7 6 6 6 5 6 6 6 5 6 5 5 5 6 6 5 5 6 5 5 5 5 5 6 5 5 6 6\n",
      " 6 5 6 5 5 5 5 5 6 6 6 5 5 5 6 6 5 5 6 6 5 5 5 6 7 5 7 6 6 6 6 6 5 6 5 5 5\n",
      " 5 6 5 6 6 6 5 5 6 6 5 5 5 7 6 5 5 6 6 5 5 5 5 6 6 5 7 6 5 7 5 6 5 5 5 5 5\n",
      " 5 6 6 6 6 6 6 7 6 7 6 5 6 6 5 6 6 6 6 6 5 5 6 5 6 5 6 6 7 5 6 6 5 5 5 7 5\n",
      " 5 5 5 6 5 6 4 4 6 6 6 5 5 5 5 5 5 5 5 5 6 5 6 5 6 5 5 5 6 7 7 6 6 5 6 5 6\n",
      " 6 5 5 6 5 5 6 6 6 5 6 6 6 6 6 6 7 5 5 5 6 5 6 5 6 6 5 6 7 6 5 6 6 6 5 5 5\n",
      " 5 5 5 5 5 5 6 6 5 5 6 5 5 6 5 5 6 6 6 7 5 5 5 5 5 6 6 5 6 5 6 6 7 7 5 6 5\n",
      " 6 6 6 5 6 5 6 6 6 6 6 5 5 5 6 5 5 5 6 7 5 6 6 6 6 5 4 5 5 6 6 5 5 6 6]\n",
      "Accuracy (Ellis implementation): 59.92\n",
      "Accuracy (Scikit-Learn implementation) 62.0\n"
     ]
    }
   ],
   "source": [
    "# Ellis implementation\n",
    "RandomForestA = RandomForest(n_trees=5)\n",
    "RandomForestA.fit(X_train, Y_train)\n",
    "RF_predictionsA = RandomForestA.predict(X_test)\n",
    "RF_accuracyA = round(RandomForestA.score(Y_test, predictions = RF_predictionsA) * 100, 2)\n",
    "\n",
    "# Scikit-Learn implementation\n",
    "RandomForestB = RandomForestClassifier(n_estimators=5)\n",
    "RandomForestB.fit(X_train, Y_train)\n",
    "RF_predictionsB = RandomForestB.predict(X_test)\n",
    "RF_accuracyB = round(RandomForestB.score(X_test, Y_test) * 100, 2)\n",
    "\n",
    "# The first print function will print out each tree in the forest (omitted)\n",
    "# Since each tree is not pruned, this will print an especially long output\n",
    "\n",
    "#print(\"Random Forest (Ellis implementation)\\n\\n\", RandomForestA)\n",
    "print(\"Predictions A\\n\", RF_predictionsA)\n",
    "print(\"Predictions B\\n\", RF_predictionsB)\n",
    "print(\"Accuracy (Ellis implementation):\", RF_accuracyA)\n",
    "print(\"Accuracy (Scikit-Learn implementation)\", RF_accuracyB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier and Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scikit-learn GaussianNB Accuracy: 0.570\n",
      "Scores: [58.307210031347964, 56.42633228840125, 55.172413793103445, 55.4858934169279, 50.78369905956113]\n",
      "Naive Bayes (from scratch) Accuracy: 55.235%\n"
     ]
    }
   ],
   "source": [
    "# Split a dataset into k folds\n",
    "def cross_validation_split(dataset, n_folds):\n",
    "\tdataset_split = list()\n",
    "\tdataset_copy = list(dataset)\n",
    "\tfold_size = int(len(dataset) / n_folds)\n",
    "\tfor _ in range(n_folds):\n",
    "\t\tfold = list()\n",
    "\t\twhile len(fold) < fold_size:\n",
    "\t\t\tindex = randrange(len(dataset_copy))\n",
    "\t\t\tfold.append(dataset_copy.pop(index))\n",
    "\t\tdataset_split.append(fold)\n",
    "\treturn dataset_split\n",
    "\n",
    "# Split the dataset by class values, returns a dictionary\n",
    "def separate_by_class(dataset):\n",
    "\tseparated = dict()\n",
    "\tfor i in range(len(dataset)):\n",
    "\t\tvector = dataset[i]\n",
    "\t\tclass_value = vector[-1]\n",
    "\t\tif (class_value not in separated):\n",
    "\t\t\tseparated[class_value] = list()\n",
    "\t\tseparated[class_value].append(vector)\n",
    "\treturn separated\n",
    " \n",
    "# Calculate the mean of a list of numbers\n",
    "def mean(numbers):\n",
    "\treturn sum(numbers)/float(len(numbers))\n",
    " \n",
    "# Calculate the standard deviation of a list of numbers\n",
    "def stdev(numbers):\n",
    "\tavg = mean(numbers)\n",
    "\tvariance = sum([(x-avg)**2 for x in numbers]) / float(len(numbers)-1)\n",
    "\treturn sqrt(variance)\n",
    " \n",
    "# Calculate the mean, stdev and count for each column in a dataset\n",
    "def summarize_dataset(dataset):\n",
    "\tsummaries = [(mean(column), stdev(column), len(column)) for column in zip(*dataset)]\n",
    "\tdel(summaries[-1])\n",
    "\treturn summaries\n",
    " \n",
    "# Split dataset by class then calculate statistics for each row\n",
    "def summarize_by_class(dataset):\n",
    "\tseparated = separate_by_class(dataset)\n",
    "\tsummaries = dict()\n",
    "\tfor class_value, rows in separated.items():\n",
    "\t\tsummaries[class_value] = summarize_dataset(rows)\n",
    "\treturn summaries\n",
    " \n",
    "# Calculate the Gaussian probability distribution function for x\n",
    "def calculate_probability(x, mean, stdev):\n",
    "\texponent = exp(-((x-mean)**2 / (2 * stdev**2 )))\n",
    "\treturn (1 / (sqrt(2 * pi) * stdev)) * exponent\n",
    " \n",
    "# Calculate the probabilities of predicting each class for a given row\n",
    "def calculate_class_probabilities(summaries, row):\n",
    "\ttotal_rows = sum([summaries[label][0][2] for label in summaries])\n",
    "\tprobabilities = dict()\n",
    "\tfor class_value, class_summaries in summaries.items():\n",
    "\t\tprobabilities[class_value] = summaries[class_value][0][2]/float(total_rows)\n",
    "\t\tfor i in range(len(class_summaries)):\n",
    "\t\t\tmean, stdev, _ = class_summaries[i]\n",
    "\t\t\tprobabilities[class_value] *= calculate_probability(row[i], mean, stdev)\n",
    "\treturn probabilities\n",
    " \n",
    "# Calculate accuracy percentage\n",
    "def accuracy_metric(actual, predicted):\n",
    "\tcorrect = 0\n",
    "\tfor i in range(len(actual)):\n",
    "\t\tif actual[i] == predicted[i]:\n",
    "\t\t\tcorrect += 1\n",
    "\treturn correct / float(len(actual)) * 100.0\n",
    " \n",
    "# Evaluate an algorithm using a cross validation split\n",
    "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n",
    "\tfolds = cross_validation_split(dataset, n_folds)\n",
    "\tscores = list()\n",
    "\tfor fold in folds:\n",
    "\t\ttrain_set = list(folds)\n",
    "\t\ttrain_set.remove(fold)\n",
    "\t\ttrain_set = sum(train_set, [])\n",
    "\t\ttest_set = list()\n",
    "\t\tfor row in fold:\n",
    "\t\t\trow_copy = list(row)\n",
    "\t\t\ttest_set.append(row_copy)\n",
    "\t\t\trow_copy[-1] = None\n",
    "\t\tpredicted = algorithm(train_set, test_set, *args)\n",
    "\t\tactual = [row[-1] for row in fold]\n",
    "\t\taccuracy = accuracy_metric(actual, predicted)\n",
    "\t\tscores.append(accuracy)\n",
    "\treturn scores\n",
    "\n",
    "# Predict the class for a given row\n",
    "def predict(summaries, row):\n",
    "\tprobabilities = calculate_class_probabilities(summaries, row)\n",
    "\tbest_label, best_prob = None, -1\n",
    "\tfor class_value, probability in probabilities.items():\n",
    "\t\tif best_label is None or probability > best_prob:\n",
    "\t\t\tbest_prob = probability\n",
    "\t\t\tbest_label = class_value\n",
    "\treturn best_label\n",
    " \n",
    "# Naive Bayes Algorithm\n",
    "def naive_bayes(train, test):\n",
    "\tsummarize = summarize_by_class(train)\n",
    "\tpredictions = list()\n",
    "\tfor row in test:\n",
    "\t\toutput = predict(summarize, row)\n",
    "\t\tpredictions.append(output)\n",
    "\treturn(predictions)\n",
    "\n",
    "NaiveBayesB = GaussianNB()\n",
    "NaiveBayesB.fit(X_train, Y_train)\n",
    "Y_pred = NaiveBayesB.predict(X_test)\n",
    "print (\"Scikit-learn GaussianNB Accuracy: {0:.3f}\".format(accuracy_score(Y_test, Y_pred)))\n",
    "\n",
    "dataset = pd.read_csv(\"../data/winequality-red-no-header.csv\")\n",
    "datalist = dataset.values.tolist()\n",
    "\n",
    "n_folds = 5\n",
    "scores = evaluate_algorithm(datalist, naive_bayes, n_folds)\n",
    "print('Scores: %s' % scores)\n",
    "print('Naive Bayes (from scratch) Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
